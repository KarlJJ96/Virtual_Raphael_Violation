{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5a4uUVcuBE-m"
      },
      "outputs": [],
      "source": [
        "!pip install pandas scikit-learn transformers torch tqdm openpyxl PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import fitz\n",
        "import re\n",
        "import string\n",
        "\n",
        "# Load the saved model and tokenizer\n",
        "model_save_path = 'bert_model.pt'\n",
        "tokenizer_save_path = 'bert_tokenizer'\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(tokenizer_save_path)\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
        "model.load_state_dict(torch.load(model_save_path))\n",
        "model.eval()\n",
        "\n",
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Function to classify text\n",
        "def classify_text(text, model, tokenizer, device):\n",
        "    encodings = tokenizer(text, truncation=True, padding=True, return_tensors='pt', max_length=512)\n",
        "    input_ids, attention_mask = encodings['input_ids'].to(device), encodings['attention_mask'].to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        prediction = torch.argmax(outputs.logits, dim=1)\n",
        "    return prediction.item()\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    document = fitz.open(pdf_path)\n",
        "    pages_text = []\n",
        "    for page_num in range(document.page_count):\n",
        "        page = document.load_page(page_num)\n",
        "        text = page.get_text(\"text\")\n",
        "        pages_text.append(text)\n",
        "    return pages_text\n",
        "\n",
        "# Function to remove all punctuation from text\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Function to remove formatting like page numbers and headers/footers\n",
        "def remove_formatting(text):\n",
        "    text = re.sub(r'Page \\d+ of \\d+', '', text)\n",
        "    text = re.sub(r'Document Code:.*\\n', '', text)\n",
        "    text = re.sub(r'Copyright.*\\n', '', text)\n",
        "    text = re.sub(r'\\n{2,}', '\\n', text)\n",
        "    return text\n",
        "\n",
        "# Function to extract relevant pages between specified headings\n",
        "def extract_relevant_pages(pages_text):\n",
        "    start_keywords = [\"Introduction\", \"Purpose\"]\n",
        "    end_keywords = [\"Appendices\", \"Appendix\"]\n",
        "    start_idx = -1\n",
        "    end_idx = len(pages_text)\n",
        "\n",
        "    # Find the start index\n",
        "    for i, text in enumerate(pages_text):\n",
        "        if any(keyword in text for keyword in start_keywords):\n",
        "            start_idx = i\n",
        "            break\n",
        "\n",
        "    # Find the end index\n",
        "    for i, text in enumerate(pages_text):\n",
        "        if any(keyword in text for keyword in end_keywords):\n",
        "            end_idx = i\n",
        "            break\n",
        "\n",
        "    if start_idx == -1:\n",
        "        start_idx = 0  # Use the entire text if no start keyword is found\n",
        "\n",
        "    return pages_text[start_idx:end_idx]\n",
        "\n",
        "# Load and process the procedure guide from a PDF file\n",
        "pdf_path = 'your_file_here.pdf'\n",
        "pages_text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "# Extract relevant pages\n",
        "relevant_pages = extract_relevant_pages(pages_text)\n",
        "\n",
        "# Classify each page and output those classified as 1\n",
        "classified_pages = []\n",
        "for page_num, page_text in enumerate(relevant_pages, start=1):\n",
        "    cleaned_text = remove_formatting(page_text)\n",
        "    clean_text = remove_punctuation(cleaned_text)\n",
        "    if clean_text.strip():  # Skip empty pages\n",
        "        prediction = classify_text(clean_text, model, tokenizer, device)\n",
        "        if prediction == 1:\n",
        "            classified_pages.append(f\"Page {page_num}\\n{page_text}\")\n",
        "\n",
        "# Save the classified pages to a text file\n",
        "output_file_path = 'High Potential Violation Directives.txt'\n",
        "with open(output_file_path, 'w') as file:\n",
        "    for page_text in classified_pages:\n",
        "        file.write(page_text + \"\\n\")\n",
        "\n",
        "print(f\"Classified pages saved to {output_file_path}\")\n"
      ],
      "metadata": {
        "id": "hE7rKKBBBKWH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}